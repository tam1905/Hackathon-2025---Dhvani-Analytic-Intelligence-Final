{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa7a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random.seed(108)\n",
    "\n",
    "# Input & output paths\n",
    "input_folder = './train/Final Train Dataset'\n",
    "working_dir = './process'\n",
    "images_dir = os.path.join(working_dir, 'images')\n",
    "annotations_dir = os.path.join(working_dir, 'annotations')\n",
    "labels_dir = os.path.join(working_dir, 'labels')\n",
    "\n",
    "for d in [working_dir, images_dir, annotations_dir, labels_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59711d4",
   "metadata": {},
   "source": [
    "2. Data Preparation: Copy & Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81b1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched images: 2987\n",
      "Matched annotations: 2987\n"
     ]
    }
   ],
   "source": [
    "# Get files\n",
    "all_files = os.listdir(input_folder)\n",
    "images = [f for f in all_files if f.lower().endswith('.jpg')]\n",
    "annotations = [f for f in all_files if f.lower().endswith('.xml')]\n",
    "\n",
    "# Copy\n",
    "for img in images:\n",
    "    shutil.copy(os.path.join(input_folder, img), os.path.join(images_dir, img))\n",
    "for ann in annotations:\n",
    "    shutil.copy(os.path.join(input_folder, ann), os.path.join(annotations_dir, ann))\n",
    "\n",
    "# Match valid pairs only\n",
    "image_names = {os.path.splitext(img)[0] for img in os.listdir(images_dir)}\n",
    "ann_names = {os.path.splitext(ann)[0] for ann in os.listdir(annotations_dir)}\n",
    "\n",
    "for img in os.listdir(images_dir):\n",
    "    if os.path.splitext(img)[0] not in ann_names:\n",
    "        os.remove(os.path.join(images_dir, img))\n",
    "\n",
    "for ann in os.listdir(annotations_dir):\n",
    "    if os.path.splitext(ann)[0] not in image_names:\n",
    "        os.remove(os.path.join(annotations_dir, ann))\n",
    "\n",
    "print(f\"Matched images: {len(os.listdir(images_dir))}\")\n",
    "print(f\"Matched annotations: {len(os.listdir(annotations_dir))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cb3c4",
   "metadata": {},
   "source": [
    "Convert Annotations (XML → YOLOv11 txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09610b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 168/2987 [00:01<00:23, 120.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted XML: ./process\\annotations\\231.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2987/2987 [00:20<00:00, 142.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pairs: 2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "selected_classes = ['car', 'bus', 'truck', 'motorbike']\n",
    "class_name_to_id_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "\n",
    "# XML parser (safe with try/except)\n",
    "def extract_info_from_xml(xml_file):\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        info_dict = {'bboxes': [], 'filename': None, 'image_size': None}\n",
    "        \n",
    "        for elem in root:\n",
    "            if elem.tag == \"filename\":\n",
    "                info_dict['filename'] = elem.text\n",
    "            elif elem.tag == \"size\":\n",
    "                w, h, d = [int(subelem.text) for subelem in elem]\n",
    "                info_dict['image_size'] = (w, h, d)\n",
    "            elif elem.tag == \"object\":\n",
    "                bbox = {}\n",
    "                for subelem in elem:\n",
    "                    if subelem.tag == \"name\" and subelem.text in selected_classes:\n",
    "                        bbox[\"class\"] = subelem.text\n",
    "                    elif subelem.tag == \"bndbox\" and \"class\" in bbox:\n",
    "                        for subsubelem in subelem:\n",
    "                            bbox[subsubelem.tag] = int(subsubelem.text)\n",
    "                if 'class' in bbox:\n",
    "                    info_dict['bboxes'].append(bbox)\n",
    "        \n",
    "        return info_dict if info_dict['filename'] else None\n",
    "    \n",
    "    except ET.ParseError:\n",
    "        print(f\"Skipping corrupted XML: {xml_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {xml_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Convert to YOLOv11\n",
    "def convert_to_yolov11(info_dict, txt_path):\n",
    "    lines = []\n",
    "    if not info_dict or not info_dict['bboxes']:\n",
    "        open(txt_path, 'w').close()\n",
    "        return\n",
    "    iw, ih, _ = info_dict['image_size']\n",
    "    for b in info_dict[\"bboxes\"]:\n",
    "        cls_id = class_name_to_id_mapping[b[\"class\"]]\n",
    "        cx = (b[\"xmin\"] + b[\"xmax\"]) / 2 / iw\n",
    "        cy = (b[\"ymin\"] + b[\"ymax\"]) / 2 / ih\n",
    "        w = (b[\"xmax\"] - b[\"xmin\"]) / iw\n",
    "        h = (b[\"ymax\"] - b[\"ymin\"]) / ih\n",
    "        lines.append(f\"{cls_id} {cx:.4f} {cy:.4f} {w:.4f} {h:.4f}\")\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "# Process all XML files\n",
    "xml_files = [os.path.join(annotations_dir, f) for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
    "valid_image_files, valid_label_files = [], []\n",
    "\n",
    "for xml_file in tqdm(xml_files):\n",
    "    info_dict = extract_info_from_xml(xml_file)\n",
    "    if info_dict:\n",
    "        img_name = info_dict['filename']\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        txt_name = img_name.replace('.jpg', '.txt')\n",
    "        txt_path = os.path.join(labels_dir, txt_name)\n",
    "        if os.path.exists(img_path):\n",
    "            convert_to_yolov11(info_dict, txt_path)\n",
    "            valid_image_files.append(img_path)\n",
    "            valid_label_files.append(txt_path)\n",
    "\n",
    "print(f\"Final pairs: {len(valid_image_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4196b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML created at ./process\\vehicle_data.yaml\n"
     ]
    }
   ],
   "source": [
    "train_imgs, temp_imgs, train_lbls, temp_lbls = train_test_split(valid_image_files, valid_label_files, test_size=0.2, random_state=42)\n",
    "val_imgs, test_imgs, val_lbls, test_lbls = train_test_split(temp_imgs, temp_lbls, test_size=0.5, random_state=42)\n",
    "\n",
    "def make_dirs(base, sub):\n",
    "    path = os.path.join(base, sub)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "train_img_dir = make_dirs(images_dir, \"train\")\n",
    "val_img_dir = make_dirs(images_dir, \"val\")\n",
    "test_img_dir = make_dirs(images_dir, \"test\")\n",
    "train_lbl_dir = make_dirs(labels_dir, \"train\")\n",
    "val_lbl_dir = make_dirs(labels_dir, \"val\")\n",
    "test_lbl_dir = make_dirs(labels_dir, \"test\")\n",
    "\n",
    "def move_files(files, dest):\n",
    "    for f in files:\n",
    "        if not os.path.exists(f): continue\n",
    "        fname = os.path.basename(f)\n",
    "        shutil.move(f, os.path.join(dest, fname))\n",
    "\n",
    "move_files(train_imgs, train_img_dir)\n",
    "move_files(val_imgs, val_img_dir)\n",
    "move_files(test_imgs, test_img_dir)\n",
    "move_files(train_lbls, train_lbl_dir)\n",
    "move_files(val_lbls, val_lbl_dir)\n",
    "move_files(test_lbls, test_lbl_dir)\n",
    "\n",
    "yaml_path = os.path.join(working_dir, 'vehicle_data.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(f\"\"\"\n",
    "train: {train_img_dir}\n",
    "val: {val_img_dir}\n",
    "test: {test_img_dir}\n",
    "\n",
    "nc: {len(selected_classes)}\n",
    "names: {selected_classes}\n",
    "\"\"\")\n",
    "print(f\"YAML created at {yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f86150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted XML: ./process\\annotations\\231.xml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = {cls: 0 for cls in selected_classes}\n",
    "areas = []\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    info = extract_info_from_xml(xml_file)\n",
    "    if info:\n",
    "        iw, ih, _ = info['image_size']\n",
    "        for b in info['bboxes']:\n",
    "            class_counts[b['class']] += 1\n",
    "            area = (b['xmax']-b['xmin']) * (b['ymax']-b['ymin'])\n",
    "            areas.append(area)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "plt.title(\"Object Counts per Class\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(np.log1p(areas), bins=30, kde=True)\n",
    "plt.title(\"Distribution of Bounding Box Areas (log scale)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583c5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root_dir):\n",
    "    train_dir = os.path.join(root_dir, 'train')\n",
    "    valid_dir = os.path.join(root_dir, 'val')\n",
    "\n",
    "    # Load training data\n",
    "    train_images, train_annotations = load_dataset(train_dir)\n",
    "\n",
    "    # Load validation data\n",
    "    valid_images, valid_annotations = load_data(valid_dir)\n",
    "\n",
    "    return train_images, train_annotations, valid_images, valid_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f14b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_data(data_dir):\n",
    "    image_paths = []\n",
    "    annotation_paths = []\n",
    "\n",
    "    # Collect image and annotation file paths\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_paths.append(os.path.join(data_dir, filename))\n",
    "            annotation_paths.append(os.path.join(data_dir, filename.replace('.jpg', '.txt').replace('.png', '.txt')))\n",
    "\n",
    "    # Load images\n",
    "    images = [cv2.imread(img_path) for img_path in image_paths]\n",
    "\n",
    "    # Load annotations\n",
    "    annotations = [load_annotations(ann_path) for ann_path in annotation_paths]\n",
    "\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7478ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_annotations(annotation_path):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Assuming YOLO format: class, x_center, y_center, width, height\n",
    "        annotations = [list(map(float, line.strip().split())) for line in lines]\n",
    "\n",
    "    return np.array(annotations)\n",
    "\n",
    "\n",
    "root_directory = r'D:\\hackthan\\dataset'\n",
    "train_images, train_annotations, valid_images, valid_annotations = load_dataset(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1aa7e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'labels', 'labels.cache']\n"
     ]
    }
   ],
   "source": [
    "train_directory = r'D:\\hackthan\\dataset\\train'\n",
    "files_in_directory = os.listdir(train_directory)\n",
    "print(files_in_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4599af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'labels', 'labels.cache']\n"
     ]
    }
   ],
   "source": [
    "valid_directory = r'D:\\hackthan\\dataset\\val'\n",
    "files_in_directory = os.listdir(valid_directory)\n",
    "print(files_in_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008ec0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'labels']\n"
     ]
    }
   ],
   "source": [
    "valid_directory = r'D:\\hackthan\\dataset\\test'\n",
    "files_in_directory = os.listdir(valid_directory)\n",
    "print(files_in_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a52dcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(data_dir):\n",
    "    image_dir = os.path.join(data_dir, 'images')\n",
    "    label_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    image_paths, label_paths = [], []\n",
    "\n",
    "    # Collect image and label file paths\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_paths.append(os.path.join(image_dir, filename))\n",
    "            label_paths.append(os.path.join(label_dir, filename.replace('.jpg', '.txt')))\n",
    "\n",
    "    return image_paths, label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e456053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(annotation_path):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Assuming YOLO format: class, x_center, y_center, width, height\n",
    "        annotations = [list(map(float, line.strip().split())) for line in lines]\n",
    "\n",
    "    return annotations\n",
    "\n",
    "image_paths, annotation_paths = load_sample_data(train_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b12ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set: 2079\n",
      "Number of labels in the training set: 2079\n"
     ]
    }
   ],
   "source": [
    "def count_files(directory):\n",
    "    image_dir = os.path.join(directory, 'images')\n",
    "    label_dir = os.path.join(directory, 'labels')\n",
    "\n",
    "    image_count = len([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "    label_count = len([f for f in os.listdir(label_dir) if f.endswith('.txt')])\n",
    "\n",
    "    return image_count, label_count\n",
    "\n",
    "image_count, label_count = count_files(train_directory)\n",
    "\n",
    "print(f\"Number of images in the training set: {image_count}\")\n",
    "print(f\"Number of labels in the training set: {label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21bf00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for D:\\hackthan\\dataset\\train\\labels\\107.txt:\n",
      "[[3.0, 0.3241, 0.614, 0.1411, 0.2147], [3.0, 0.6839, 0.5613, 0.0643, 0.1413]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\108.txt:\n",
      "[[1.0, 0.2402, 0.6173, 0.4768, 0.76]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\109.txt:\n",
      "[]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\112.txt:\n",
      "[[1.0, 0.7826, 0.4292, 0.4347, 0.6792], [0.0, 0.4111, 0.424, 0.1361, 0.0813], [3.0, 0.2403, 0.513, 0.1111, 0.0969], [1.0, 0.1382, 0.3818, 0.1792, 0.1219], [0.0, 0.5389, 0.399, 0.0694, 0.0396], [1.0, 0.4986, 0.3667, 0.1, 0.0521], [1.0, 0.3111, 0.3594, 0.0833, 0.0292]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\116.txt:\n",
      "[[1.0, 0.5111, 0.4401, 0.8278, 0.7531]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\117.txt:\n",
      "[[0.0, 0.1951, 0.5375, 0.3875, 0.2375], [0.0, 0.7215, 0.4411, 0.1819, 0.1073], [3.0, 0.6042, 0.5042, 0.1222, 0.2292], [1.0, 0.3438, 0.3411, 0.1931, 0.051], [1.0, 0.7757, 0.3641, 0.1514, 0.0969], [0.0, 0.6493, 0.3729, 0.1181, 0.0333], [1.0, 0.5215, 0.3578, 0.0875, 0.0385], [1.0, 0.0576, 0.3495, 0.0597, 0.0219]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\206.txt:\n",
      "[[3.0, 0.6386, 0.5736, 0.0486, 0.0899], [3.0, 0.7021, 0.5777, 0.0557, 0.1417], [3.0, 0.9293, 0.5409, 0.0529, 0.1063], [3.0, 0.5793, 0.7793, 0.0986, 0.1907], [0.0, 0.5986, 0.7098, 0.1857, 0.1935], [0.0, 0.3057, 0.827, 0.2543, 0.2643], [0.0, 0.4857, 0.4605, 0.1314, 0.109], [0.0, 0.4229, 0.5627, 0.1457, 0.1608], [0.0, 0.5964, 0.5136, 0.1443, 0.1226], [0.0, 0.76, 0.4278, 0.1371, 0.1526], [0.0, 0.1664, 0.4305, 0.0871, 0.0817], [0.0, 0.1636, 0.3447, 0.0614, 0.0845], [0.0, 0.0914, 0.391, 0.0571, 0.0845], [0.0, 0.205, 0.3787, 0.0614, 0.0926]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\207.txt:\n",
      "[[3.0, 0.9085, 0.5606, 0.045, 0.268]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\208.txt:\n",
      "[[0.0, 0.675, 0.9139, 0.1354, 0.1722], [1.0, 0.7427, 0.6935, 0.1458, 0.2981], [1.0, 0.8797, 0.525, 0.1281, 0.2759], [1.0, 0.0542, 0.7046, 0.1062, 0.3833], [1.0, 0.9734, 0.463, 0.0531, 0.1778], [0.0, 0.1557, 0.8676, 0.1135, 0.1759], [0.0, 0.2411, 0.8037, 0.1031, 0.1593], [0.0, 0.3323, 0.7065, 0.0979, 0.1648], [0.0, 0.2802, 0.5917, 0.0875, 0.1204], [1.0, 0.2167, 0.5056, 0.1062, 0.2444], [1.0, 0.4531, 0.5417, 0.1, 0.2315], [0.0, 0.3745, 0.5083, 0.0635, 0.087], [0.0, 0.5792, 0.4583, 0.0625, 0.1019], [1.0, 0.4661, 0.3259, 0.1073, 0.1407], [1.0, 0.4349, 0.3611, 0.076, 0.1259], [1.0, 0.3573, 0.4472, 0.0896, 0.1722], [1.0, 0.7292, 0.4417, 0.1083, 0.1648]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\train\\labels\\209.txt:\n",
      "[[1.0, 0.7056, 0.7972, 0.1533, 0.4055], [0.0, 0.4867, 0.9321, 0.1333, 0.1359], [0.0, 0.3756, 0.9151, 0.1178, 0.1699], [0.0, 0.4644, 0.7919, 0.1222, 0.1826], [0.0, 0.5839, 0.8238, 0.11, 0.2081], [0.0, 0.8811, 0.7187, 0.0711, 0.1295], [0.0, 0.8494, 0.6295, 0.0678, 0.0998], [0.0, 0.5672, 0.6019, 0.0589, 0.1083], [0.0, 0.7439, 0.5924, 0.0567, 0.034], [0.0, 0.8178, 0.5775, 0.0556, 0.1019], [0.0, 0.5839, 0.5446, 0.0522, 0.087], [0.0, 0.6467, 0.5764, 0.0578, 0.0573], [0.0, 0.7406, 0.5499, 0.0567, 0.0637], [0.0, 0.7672, 0.5255, 0.0656, 0.0955], [0.0, 0.6772, 0.5414, 0.0567, 0.0764], [0.0, 0.6344, 0.5117, 0.0511, 0.0807], [0.0, 0.595, 0.4777, 0.0478, 0.0637], [0.0, 0.6939, 0.4894, 0.0522, 0.0531], [0.0, 0.6589, 0.4522, 0.0533, 0.051], [0.0, 0.6261, 0.4289, 0.0433, 0.0637], [0.0, 0.7083, 0.4575, 0.0433, 0.0488], [0.0, 0.7583, 0.4682, 0.05, 0.0531], [0.0, 0.8194, 0.4915, 0.0478, 0.0913], [0.0, 0.6372, 0.3811, 0.0322, 0.0361], [0.0, 0.6778, 0.3769, 0.0289, 0.0531], [0.0, 0.7022, 0.3896, 0.0267, 0.0573], [1.0, 0.7833, 0.2643, 0.0978, 0.0488], [0.0, 0.8067, 0.3079, 0.0289, 0.0297], [1.0, 0.675, 0.2633, 0.0411, 0.0425], [0.0, 0.1956, 0.7919, 0.1111, 0.1614], [1.0, 0.1289, 0.6115, 0.1889, 0.276], [0.0, 0.2478, 0.6699, 0.0733, 0.138], [0.0, 0.3583, 0.6794, 0.0722, 0.1104], [0.0, 0.4017, 0.6072, 0.0878, 0.0977], [0.0, 0.2417, 0.5584, 0.0522, 0.0892], [0.0, 0.3506, 0.5828, 0.0367, 0.0828], [0.0, 0.3744, 0.5276, 0.06, 0.0786], [0.0, 0.3606, 0.4766, 0.0678, 0.0786], [1.0, 0.5389, 0.2856, 0.0533, 0.0488]]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print labels for the first 10 samples\n",
    "for i in range(min(10, len(annotation_paths))):\n",
    "    sample_annotations = load_annotations(annotation_paths[i])\n",
    "    print(f\"Labels for {annotation_paths[i]}:\")\n",
    "    print(sample_annotations)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58967095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for D:\\hackthan\\dataset\\test\\labels\\229.txt:\n",
      "[[0.0, 0.3657, 0.6981, 0.1782, 0.1349], [1.0, 0.6134, 0.5096, 0.1181, 0.1073], [0.0, 0.1157, 0.8261, 0.2292, 0.2314], [0.0, 0.3646, 0.5885, 0.1065, 0.0751], [0.0, 0.3796, 0.5303, 0.0903, 0.0536], [0.0, 0.0544, 0.6084, 0.1065, 0.072], [3.0, 0.0185, 0.5513, 0.0347, 0.0467], [0.0, 0.1759, 0.5149, 0.0579, 0.0352], [1.0, 0.4815, 0.4391, 0.0394, 0.0368], [0.0, 0.3727, 0.4797, 0.0486, 0.0322], [3.0, 0.2025, 0.4759, 0.0278, 0.0276]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\230.txt:\n",
      "[[1.0, 0.3944, 0.5856, 0.1211, 0.2541], [0.0, 0.0559, 0.8204, 0.1087, 0.1713], [1.0, 0.3315, 0.5262, 0.1289, 0.268], [1.0, 0.4472, 0.3066, 0.0559, 0.0773], [1.0, 0.4402, 0.3895, 0.0637, 0.0939], [0.0, 0.257, 0.6547, 0.0761, 0.0884], [0.0, 0.927, 0.7624, 0.1118, 0.1215], [0.0, 0.177, 0.5152, 0.059, 0.058], [0.0, 0.0512, 0.7099, 0.0901, 0.0442], [0.0, 0.6095, 0.6644, 0.0792, 0.0746], [0.0, 0.1056, 0.4959, 0.0621, 0.047], [0.0, 0.2616, 0.3453, 0.0419, 0.0331]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_08.txt:\n",
      "[[1.0, 0.5581, 0.5824, 0.4932, 0.4611], [1.0, 0.8771, 0.4926, 0.176, 0.187], [1.0, 0.9714, 0.5106, 0.0573, 0.1787], [1.0, 0.1628, 0.512, 0.1422, 0.0944], [1.0, 0.2807, 0.5037, 0.0833, 0.0778], [0.0, 0.2914, 0.5852, 0.0464, 0.0667], [0.0, 0.2195, 0.5889, 0.088, 0.063], [0.0, 0.149, 0.5819, 0.0448, 0.0602]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_14.txt:\n",
      "[[0.0, 0.8638, 0.4565, 0.2672, 0.3907], [0.0, 0.8659, 0.3319, 0.2557, 0.2361], [1.0, 0.9682, 0.1097, 0.0635, 0.1472], [1.0, 0.1294, 0.1361, 0.2578, 0.1741]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_19.txt:\n",
      "[[1.0, 0.1182, 0.2005, 0.2354, 0.2639], [0.0, 0.5086, 0.2222, 0.0797, 0.0815], [0.0, 0.9383, 0.2176, 0.0328, 0.0648], [0.0, 0.9773, 0.2116, 0.0453, 0.0639]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_26.txt:\n",
      "[[1.0, 0.7245, 0.4056, 0.1542, 0.0926], [0.0, 0.2573, 0.481, 0.1021, 0.0657], [0.0, 0.1198, 0.4718, 0.1281, 0.0583], [0.0, 0.4094, 0.7921, 0.476, 0.4157], [0.0, 0.9039, 0.5861, 0.1922, 0.1852], [1.0, 0.9422, 0.6852, 0.1156, 0.6148]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_30.txt:\n",
      "[[1.0, 0.9508, 0.2412, 0.0984, 0.2398], [1.0, 0.1995, 0.1542, 0.2354, 0.1917], [1.0, 0.3706, 0.2037, 0.1005, 0.0926], [1.0, 0.5445, 0.1731, 0.1234, 0.0889], [1.0, 0.6586, 0.1435, 0.0776, 0.0907], [1.0, 0.7, 0.1597, 0.0604, 0.0472], [1.0, 0.0464, 0.2023, 0.0708, 0.125], [1.0, 0.2807, 0.8287, 0.5604, 0.3426], [0.0, 0.0797, 0.7069, 0.1573, 0.2157], [0.0, 0.4753, 0.6407, 0.2526, 0.2722], [3.0, 0.8633, 0.4861, 0.0401, 0.1093], [3.0, 0.8893, 0.381, 0.0307, 0.062], [0.0, 0.2026, 0.3667, 0.2073, 0.1056], [0.0, 0.3307, 0.4032, 0.1698, 0.1343], [0.0, 0.6281, 0.4278, 0.1042, 0.1833], [0.0, 0.5799, 0.3282, 0.0964, 0.1157], [0.0, 0.5461, 0.2815, 0.0901, 0.0833], [0.0, 0.6638, 0.2829, 0.0714, 0.0769], [0.0, 0.6339, 0.2542, 0.0677, 0.0657], [0.0, 0.7438, 0.2639, 0.0563, 0.0741], [0.0, 0.7063, 0.2491, 0.05, 0.0889], [0.0, 0.7424, 0.219, 0.037, 0.0417], [0.0, 0.0281, 0.3093, 0.0542, 0.0593], [0.0, 0.0729, 0.2843, 0.1198, 0.0852], [0.0, 0.199, 0.3139, 0.1615, 0.0833], [0.0, 0.1867, 0.2551, 0.0839, 0.0491], [0.0, 0.2536, 0.2722, 0.0604, 0.0722], [0.0, 0.8661, 0.2741, 0.0542, 0.0796], [0.0, 0.8021, 0.2486, 0.0417, 0.062], [0.0, 0.8883, 0.2394, 0.0286, 0.0657], [0.0, 0.8643, 0.2222, 0.0193, 0.0593], [1.0, 0.75, 0.1625, 0.0312, 0.0231], [1.0, 0.0115, 0.1787, 0.0219, 0.2704], [0.0, 0.6682, 0.2796, 0.0708, 0.0685], [0.0, 0.4727, 0.2306, 0.0432, 0.0296], [0.0, 0.5161, 0.2329, 0.0542, 0.0454], [0.0, 0.826, 0.2125, 0.026, 0.0509], [0.0, 0.8992, 0.194, 0.0193, 0.0361]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_33.txt:\n",
      "[[1.0, 0.8667, 0.7315, 0.2667, 0.5296], [1.0, 0.7529, 0.5009, 0.1724, 0.387], [1.0, 0.6661, 0.2676, 0.1052, 0.15], [0.0, 0.4065, 0.9167, 0.162, 0.1667], [0.0, 0.3271, 0.5625, 0.1135, 0.2213], [0.0, 0.4487, 0.4194, 0.0766, 0.1815], [0.0, 0.4583, 0.3014, 0.0573, 0.1231], [0.0, 0.3943, 0.2903, 0.049, 0.0806], [0.0, 0.4, 0.2352, 0.0469, 0.0778], [0.0, 0.4615, 0.2259, 0.051, 0.0852], [0.0, 0.4211, 0.1968, 0.0318, 0.0676], [0.0, 0.4656, 0.1824, 0.0344, 0.0593], [1.0, 0.6034, 0.4477, 0.1714, 0.5065], [0.0, 0.4354, 0.1477, 0.0292, 0.0454], [3.0, 0.4099, 0.1412, 0.0135, 0.0287], [0.0, 0.4727, 0.1384, 0.0307, 0.0491], [1.0, 0.5977, 0.162, 0.0766, 0.1037], [1.0, 0.5823, 0.1065, 0.0583, 0.0926], [1.0, 0.5365, 0.1241, 0.0458, 0.1185], [1.0, 0.5471, 0.0523, 0.0391, 0.0509], [1.0, 0.5151, 0.0278, 0.0292, 0.0537], [1.0, 0.4732, 0.0273, 0.0297, 0.0509], [1.0, 0.4862, 0.0699, 0.0245, 0.0602], [0.0, 0.444, 0.0597, 0.0161, 0.025], [0.0, 0.4388, 0.106, 0.0286, 0.0306], [0.0, 0.1781, 0.4403, 0.1177, 0.1398], [0.0, 0.0677, 0.4801, 0.1344, 0.1398], [0.0, 0.0414, 0.4319, 0.0786, 0.1046], [0.0, 0.1568, 0.375, 0.1052, 0.1111], [0.0, 0.0893, 0.3407, 0.0932, 0.0981], [0.0, 0.0372, 0.281, 0.0682, 0.0398], [0.0, 0.1633, 0.2023, 0.0495, 0.0565], [0.0, 0.2516, 0.3287, 0.0865, 0.1056], [0.0, 0.2979, 0.2657, 0.0573, 0.1], [0.0, 0.2057, 0.2912, 0.0417, 0.0824], [0.0, 0.2557, 0.244, 0.0479, 0.0954], [0.0, 0.2786, 0.1866, 0.0448, 0.0583], [0.0, 0.2562, 0.1569, 0.0344, 0.0454], [0.0, 0.3594, 0.156, 0.0323, 0.0435], [0.0, 0.3312, 0.1431, 0.026, 0.0398], [1.0, 0.2698, 0.0782, 0.0458, 0.0546], [0.0, 0.3768, 0.1208, 0.0224, 0.0491], [0.0, 0.4159, 0.0634, 0.0172, 0.025], [0.0, 0.3802, 0.0852, 0.0198, 0.0296]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_53.txt:\n",
      "[[3.0, 0.3359, 0.7787, 0.0615, 0.1241], [3.0, 0.3805, 0.7449, 0.0234, 0.0713], [0.0, 0.4216, 0.7097, 0.0474, 0.0769], [0.0, 0.675, 0.7523, 0.1167, 0.1509], [0.0, 0.4518, 0.7278, 0.0339, 0.1019], [0.0, 0.8594, 0.8148, 0.2771, 0.3704], [1.0, 0.8659, 0.6269, 0.1474, 0.0963], [0.0, 0.8094, 0.7139, 0.1052, 0.1]]\n",
      "------------------------\n",
      "Labels for D:\\hackthan\\dataset\\test\\labels\\Asraf_59.txt:\n",
      "[[0.0, 0.4151, 0.725, 0.0583, 0.0963], [0.0, 0.8164, 0.8324, 0.3672, 0.3185], [0.0, 0.6492, 0.7333, 0.0349, 0.0907]]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "image_paths, annotation_paths = load_sample_data(valid_directory)\n",
    "\n",
    "\n",
    "for i in range(min(10, len(annotation_paths))):\n",
    "    sample_annotations = load_annotations(annotation_paths[i])\n",
    "    print(f\"Labels for {annotation_paths[i]}:\")\n",
    "    print(sample_annotations)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e1981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(data_dir):\n",
    "    image_dir = os.path.join(data_dir, 'images')\n",
    "    label_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    image_paths, label_paths = [], []\n",
    "\n",
    "    # Collect image and label file paths\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_paths.append(os.path.join(image_dir, filename))\n",
    "            label_paths.append(os.path.join(label_dir, filename.replace('.jpg', '.txt')))\n",
    "\n",
    "    return image_paths, label_paths\n",
    "\n",
    "def load_annotations(annotation_path):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Assuming YOLO format: class, x_center, y_center, width, height\n",
    "        annotations = [list(map(float, line.strip().split())) for line in lines]\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d90bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def display_sample(image_path, annotations):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        class_label, x_center, y_center, width, height = annotation\n",
    "        x, y, w, h = x_center, y_center, width, height\n",
    "\n",
    "        # Convert YOLO coordinates to bounding box coordinates\n",
    "        x *= image.shape[1]\n",
    "        y *= image.shape[0]\n",
    "        w *= image.shape[1]\n",
    "        h *= image.shape[0]\n",
    "\n",
    "        # Calculate top-left corner coordinates\n",
    "        x -= w / 2\n",
    "        y -= h / 2\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Display class label\n",
    "        plt.text(x, y, f'{int(class_label)}', color='r')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_paths, annotation_paths = load_sample_data(train_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "709107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e78f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ae8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ddd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.204 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.160  Python-3.11.9 torch-2.7.1+cpu CPU (12th Gen Intel Core(TM) i5-12400)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=I:\\2025\\hackthan\\process\\vehicle_data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,620 parameters, 2,590,604 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.40.3 ms, read: 11.92.1 MB/s, size: 322.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning I:\\2025\\hackthan\\dataset\\train\\labels.cache... 2079 images, 255 backgrounds, 106 corrupt: 100%|██████████| 2291/2291 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_ 330.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_ 330.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_344.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_344.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_345.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_345.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_346.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_346.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_349.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_349.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_525.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_525.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_526.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_526.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_527.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_527.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_528.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_528.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_529.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_529.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_530.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_530.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_531.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_531.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_532.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_532.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_533.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_533.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_534.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_534.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_535.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_535.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_537.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_537.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_538.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_538.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_539.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_539.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_540.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_540.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_541.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_541.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_543.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_543.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Dipto_544.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Dipto_544.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_352.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_352.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_355.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_355.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_395.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_395.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_579.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_579.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_580.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_580.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_581.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_581.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_582.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_582.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_583.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_583.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_586.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_586.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_587.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_587.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_588.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_588.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_589.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_589.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_590.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_590.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_591.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_591.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_592.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_592.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_593.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_593.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_594.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_594.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_596.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_596.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_597.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_597.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_598.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_598.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_599.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_599.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_600.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_600.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_601.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_601.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_602.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_602.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_603.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_603.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_604.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_604.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_605.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_605.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_608.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_608.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_609.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_609.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_610.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_610.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_611.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_611.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_612.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_612.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_613.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_613.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_614.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_614.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_615.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_615.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_616.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_616.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_617.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_617.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_618.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_618.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_619.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_619.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_621.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_621.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_622.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_622.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_628.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_628.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_629.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_629.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_631.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_631.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_632.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_632.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_633.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_633.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_634.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_634.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_635.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_635.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_636.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_636.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_637.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_637.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_639.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_639.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_640.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_640.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_641.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_641.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_642.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_642.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_643.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_643.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_644.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_644.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_645.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_645.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_646.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_646.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_647.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_647.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_648.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_648.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_649.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_649.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_651.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_651.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_652.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_652.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_655.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_655.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_656.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_656.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_657.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_657.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_659.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_659.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_660.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_660.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Navid_662.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Navid_662.JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(295).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(295).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(297).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(297).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(298).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(298).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(299).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(299).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(300).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(300).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(301).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(301).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(303).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(303).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(304).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(304).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(305).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(305).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(306).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(306).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(307).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(307).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(308).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(308).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(309).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(309).JPG'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mI:\\2025\\hackthan\\dataset\\train\\labels\\Numan_(310).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\train\\\\labels\\\\Numan_(310).JPG'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 10.16.2 MB/s, size: 523.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning I:\\2025\\hackthan\\dataset\\val\\labels.cache... 259 images, 30 backgrounds, 14 corrupt: 100%|██████████| 287/287 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Dipto_348.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Dipto_348.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Dipto_536.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Dipto_536.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_230.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_230.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_578.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_578.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_584.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_584.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_585.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_585.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_595.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_595.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_607.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_607.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_626.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_626.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_630.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_630.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_638.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_638.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_650.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_650.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Navid_653.JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Navid_653.JPG'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mI:\\2025\\hackthan\\dataset\\val\\labels\\Numan_(302).JPG: ignoring corrupt image/label: cannot identify image file 'I:\\\\2025\\\\hackthan\\\\dataset\\\\val\\\\labels\\\\Numan_(302).JPG'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.391      2.723      1.177         36        640: 100%|██████████| 137/137 [08:40<00:00,  3.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:26<00:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.548      0.307      0.371      0.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.434      1.994      1.205         50        640: 100%|██████████| 137/137 [05:52<00:00,  2.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:13<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897       0.54      0.384      0.404      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.472      1.918      1.236         89        640: 100%|██████████| 137/137 [05:51<00:00,  2.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:13<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.472      0.393      0.391      0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.469      1.825      1.231         95        640: 100%|██████████| 137/137 [05:56<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.493      0.432      0.421      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.424      1.615      1.214         85        640: 100%|██████████| 137/137 [05:54<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:13<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.527      0.469      0.452      0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.387      1.555      1.193         50        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.604      0.489      0.509      0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.365      1.458       1.18         29        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897       0.52      0.498      0.498       0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.348      1.392       1.16         55        640: 100%|██████████| 137/137 [05:56<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.552      0.501      0.509      0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G      1.344      1.373      1.156         69        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.594      0.503       0.53      0.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      1.317      1.342       1.15         68        640: 100%|██████████| 137/137 [05:53<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.661      0.507      0.562      0.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.289      1.268      1.136         45        640: 100%|██████████| 137/137 [05:55<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.605      0.513      0.549       0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.276      1.254       1.13         92        640: 100%|██████████| 137/137 [05:58<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.652       0.51      0.567      0.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.273      1.238      1.119         18        640: 100%|██████████| 137/137 [05:59<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.629      0.551      0.591      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.256      1.209       1.12         81        640: 100%|██████████| 137/137 [05:57<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.642      0.546      0.595      0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.246      1.196      1.111         51        640: 100%|██████████| 137/137 [05:59<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.693      0.535      0.599      0.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.233       1.15      1.097         63        640: 100%|██████████| 137/137 [05:58<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.648      0.503      0.569      0.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.227      1.144      1.102         72        640: 100%|██████████| 137/137 [05:56<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.641       0.55      0.584      0.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.201      1.107      1.091         60        640: 100%|██████████| 137/137 [05:58<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.693      0.559      0.627      0.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      1.177      1.086      1.079         43        640: 100%|██████████| 137/137 [06:01<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.687      0.572      0.621      0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.161      1.076      1.078         65        640: 100%|██████████| 137/137 [05:59<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897       0.68      0.581      0.628      0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G       1.19      1.078      1.077        103        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.686      0.595      0.648      0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.181      1.054      1.079         88        640: 100%|██████████| 137/137 [05:58<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.679      0.589      0.645      0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.162      1.035       1.07         45        640: 100%|██████████| 137/137 [06:01<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.714      0.601      0.673       0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.142      1.001      1.059         45        640: 100%|██████████| 137/137 [05:57<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.743      0.581      0.672       0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G      1.143     0.9974      1.056         54        640: 100%|██████████| 137/137 [05:57<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.733      0.585      0.675      0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.126     0.9833      1.055         48        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.694      0.599      0.665      0.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.126     0.9649      1.042         41        640: 100%|██████████| 137/137 [05:55<00:00,  2.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.671      0.571      0.649      0.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.113     0.9584      1.042         48        640: 100%|██████████| 137/137 [06:01<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.745       0.58      0.684      0.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.112     0.9416      1.043         67        640: 100%|██████████| 137/137 [05:58<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.738      0.592      0.683      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      1.097     0.9278      1.037         71        640: 100%|██████████| 137/137 [06:01<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.746        0.6      0.698      0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      1.076     0.9074      1.029         67        640: 100%|██████████| 137/137 [05:59<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.715      0.623       0.69      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      1.089     0.9017      1.031         38        640: 100%|██████████| 137/137 [06:02<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.763      0.616      0.698      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G       1.09     0.9032      1.032         63        640: 100%|██████████| 137/137 [05:59<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.709      0.594      0.687      0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      1.069     0.8824      1.028         75        640: 100%|██████████| 137/137 [06:03<00:00,  2.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.723      0.613      0.696      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G       1.06     0.8698      1.024         73        640: 100%|██████████| 137/137 [05:52<00:00,  2.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.725      0.619      0.704      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      1.053     0.8593      1.016         62        640: 100%|██████████| 137/137 [05:50<00:00,  2.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.752      0.618       0.71      0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G       1.04     0.8453      1.012        130        640: 100%|██████████| 137/137 [05:49<00:00,  2.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.746      0.586       0.69      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      1.033     0.8351      1.009         38        640: 100%|██████████| 137/137 [05:56<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.723       0.62      0.706      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G      1.017     0.8146      1.004         54        640: 100%|██████████| 137/137 [05:54<00:00,  2.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.712      0.647      0.713       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G      1.031     0.8238       1.01         52        640: 100%|██████████| 137/137 [05:53<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.738      0.647      0.725      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      41/50         0G      1.015     0.7733     0.9843         21        640: 100%|██████████| 137/137 [05:46<00:00,  2.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.707      0.639      0.698      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      1.004     0.7384     0.9882         43        640: 100%|██████████| 137/137 [05:47<00:00,  2.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.722      0.652      0.714      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G     0.9923     0.7288     0.9851         26        640: 100%|██████████| 137/137 [05:48<00:00,  2.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.724      0.637      0.714      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G     0.9808     0.7223     0.9727         83        640: 100%|██████████| 137/137 [05:47<00:00,  2.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.758      0.617      0.713      0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G     0.9655     0.6948     0.9723         28        640: 100%|██████████| 137/137 [05:49<00:00,  2.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.729       0.64      0.713      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G     0.9636      0.689     0.9648         29        640: 100%|██████████| 137/137 [05:49<00:00,  2.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897       0.74      0.656      0.714      0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G     0.9508     0.6814     0.9675         33        640: 100%|██████████| 137/137 [05:46<00:00,  2.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.758      0.666       0.73      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G     0.9536     0.6716     0.9587         25        640: 100%|██████████| 137/137 [05:47<00:00,  2.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.756      0.634      0.723      0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.9342     0.6618     0.9553         31        640: 100%|██████████| 137/137 [05:49<00:00,  2.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.754      0.648      0.727      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.9335      0.658     0.9503         35        640: 100%|██████████| 137/137 [05:49<00:00,  2.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.764      0.646      0.728       0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 5.203 hours.\n",
      "Optimizer stripped from runs\\detect\\train2\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train2\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.160  Python-3.11.9 torch-2.7.1+cpu CPU (12th Gen Intel Core(TM) i5-12400)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        273        897      0.759      0.666       0.73      0.484\n",
      "                   car        138        338      0.711      0.716      0.752      0.547\n",
      "                   bus        130        248       0.78       0.73      0.774      0.549\n",
      "                 truck         83        155      0.777      0.673      0.732       0.52\n",
      "             motorbike         88        156      0.768      0.545       0.66      0.321\n",
      "Speed: 0.5ms preprocess, 26.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results=model.train(data=r\"D:\\hackthan\\process\\vehicle_data.yaml\",epochs=50,imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e4a94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d911c1e8083147f78d264cdd9aa344df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6201f9ce739e46c9b8429bebe0a574ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FileUpload, Output\n",
    "from IPython.display import display, Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# Create file upload widget\n",
    "file_upload = FileUpload(accept='image/*', multiple=False)  # Only accept image files\n",
    "output = Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    # Clear previous output\n",
    "    output.clear_output()\n",
    "    \n",
    "    # Retrieve the uploaded file\n",
    "    if isinstance(file_upload.value, dict):  # Check if value is a dict (older versions)\n",
    "        uploaded_file = list(file_upload.value.values())[0]\n",
    "        content = uploaded_file['content']\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "    elif isinstance(file_upload.value, tuple):  # Check for tuple (newer versions)\n",
    "        uploaded_file = file_upload.value[0]\n",
    "        content = uploaded_file['content']\n",
    "        filename = uploaded_file['name']\n",
    "    else:\n",
    "        with output:\n",
    "            print(\"Unrecognized file_upload structure!\")\n",
    "        return\n",
    "    \n",
    "    # Save the uploaded file temporarily\n",
    "    temp_path = f\"./{filename}\"\n",
    "    with open(temp_path, 'wb') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    # Load the image with OpenCV\n",
    "    image = cv2.imdecode(np.frombuffer(content, np.uint8), cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Perform YOLO prediction\n",
    "    results = model.predict(source=image, show=False)  # Disable display in this function\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            class_id = int(box.cls[0])  # Class ID\n",
    "            class_name = model.names[class_id]  # Get class name from ID\n",
    "            confidence = box.conf[0]  # Confidence score\n",
    "            \n",
    "            # Draw the bounding box and label\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box\n",
    "            label = f\"{class_name} ({confidence:.2f})\"\n",
    "            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display detected classes and the image with bounding boxes\n",
    "    with output:\n",
    "        print(\"Detected Classes:\")\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                class_id = int(box.cls[0])  # Class ID\n",
    "                class_name = model.names[class_id]  # Get class name from ID\n",
    "                print(class_name)\n",
    "        \n",
    "        # Convert image to JPEG for display\n",
    "        _, encoded_img = cv2.imencode('.jpg', image)\n",
    "        display(Image(data=encoded_img.tobytes(), format='jpg'))\n",
    "\n",
    "# Attach event listener to the file upload widget\n",
    "file_upload.observe(on_upload_change, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(file_upload, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137eadc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
